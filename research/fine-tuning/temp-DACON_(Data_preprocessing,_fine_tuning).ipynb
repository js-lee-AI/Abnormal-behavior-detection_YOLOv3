{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DACON (Data preprocessing, fine-tuning).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3FzuugT7rJ90dCYfdKPCW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/js-lee-AI/Abnormal-detection/blob/master/research/fine-tuning/temp-DACON_(Data_preprocessing%2C_fine_tuning).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbp3LqMO9xal"
      },
      "source": [
        "# Install and Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVlPKwpI9wWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af6bdd5-26ce-4d77-b9e4-adf25b9fad47"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install transformers pytorch-lightning sentencepiece datasets\n",
        "!pip install tqdm==4.43.0\n",
        "!pip install tokenizers==0.10.3\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\n",
            "\u001b[K     |████████████████████████████████| 813 kB 40.0 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 43.8 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.10.2-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 22.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 43.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Collecting pyDeprecate==0.3.0\n",
            "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 51.5 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.2.0\n",
            "  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 47.1 MB/s \n",
            "\u001b[?25hCollecting tensorboard!=2.5.0,>=2.2.0\n",
            "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6 MB 43.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (7.1.2)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.9.0+cu102)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 47.9 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 34.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.34.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.32.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (57.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting tqdm>=4.27\n",
            "  Downloading tqdm-4.61.2-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 48.8 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 50.6 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=0712ab7b9f5436c29a7a8ec7bf3a1ebd3cf8069079064ee763c9d71ef6f2c860\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, yarl, async-timeout, tqdm, fsspec, aiohttp, xxhash, torchmetrics, tokenizers, tensorboard, sacremoses, pyyaml, pyDeprecate, huggingface-hub, future, transformers, sentencepiece, pytorch-lightning, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.5.0 requires tensorboard~=2.5, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.10.2 fsspec-2021.7.0 future-0.18.2 huggingface-hub-0.0.12 multidict-5.1.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.8 pyyaml-5.4.1 sacremoses-0.0.45 sentencepiece-0.1.96 tensorboard-2.4.1 tokenizers-0.10.3 torchmetrics-0.4.1 tqdm-4.61.2 transformers-4.9.0 xxhash-2.0.2 yarl-1.6.3\n",
            "Collecting tqdm==4.43.0\n",
            "  Downloading tqdm-4.43.0-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 3.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.61.2\n",
            "    Uninstalling tqdm-4.61.2:\n",
            "      Successfully uninstalled tqdm-4.61.2\n",
            "Successfully installed tqdm-4.43.0\n",
            "Requirement already satisfied: tokenizers==0.10.3 in /usr/local/lib/python3.7/dist-packages (0.10.3)\n",
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-hwloorw2/kobert-tokenizer_122cd29c35c14419936a9fd9cdf591ec\n",
            "  Running command git clone -q https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-hwloorw2/kobert-tokenizer_122cd29c35c14419936a9fd9cdf591ec\n",
            "Building wheels for collected packages: kobert-tokenizer\n",
            "  Building wheel for kobert-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert-tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4627 sha256=ce674733c0592357d94fca7089a7bd7615852ec75d10fbaf09001fe8582d822d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b5jd5l7k/wheels/10/b4/d9/cb627bbfaefa266657b0b4e8127f7bf96d27376fa1a23897b4\n",
            "Successfully built kobert-tokenizer\n",
            "Installing collected packages: kobert-tokenizer\n",
            "Successfully installed kobert-tokenizer-0.1\n",
            "\u001b[31mERROR: torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu102)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSU8TB2PAv4i"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "from torch.utils.data import DataLoader\n",
        "# from datasets import Dataset, DatasetDict\n",
        "from sklearn.metrics import log_loss, accuracy_score,f1_score\n",
        "\n",
        "from pytorch_lightning import LightningDataModule\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification \n",
        "\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import tqdm\n",
        "import argparse"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJszzI4t3Zis"
      },
      "source": [
        "from typing import List, Optional, Union, Tuple\n",
        "from tokenizers import Tokenizer, decoders, pre_tokenizers, AddedToken\n",
        "from tokenizers.implementations import BaseTokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.normalizers import NFKC\n",
        "import torch\n",
        "\n",
        "\n",
        "class BrainBertTokenizer(BaseTokenizer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab: Union[str, List],\n",
        "        merges: List[Tuple[str, str]],\n",
        "        bos_token: str = \"<s>\",\n",
        "        eos_token: str = \"</s>\",\n",
        "        sep_token: str = \"</s>\",\n",
        "        cls_token: str = \"<s>\",\n",
        "        pad_token: str = \"<pad>\",\n",
        "        unk_token: str = \"<unk>\",\n",
        "        replacement: str = \"▁\",\n",
        "        add_prefix_space: bool = True,\n",
        "        dropout: Optional[float] = None,\n",
        "        normalize: bool = True,\n",
        "    ):\n",
        "        bpe = BPE(\n",
        "            vocab=vocab,\n",
        "            merges=merges,\n",
        "            unk_token=unk_token,\n",
        "            fuse_unk=True,\n",
        "        )\n",
        "\n",
        "        tokenizer = Tokenizer(bpe)\n",
        "\n",
        "        tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(\n",
        "            replacement=replacement,\n",
        "            add_prefix_space=add_prefix_space,\n",
        "        )\n",
        "\n",
        "        tokenizer.decoder = decoders.Metaspace(\n",
        "            replacement=replacement,\n",
        "            add_prefix_space=add_prefix_space,\n",
        "        )\n",
        "\n",
        "        if normalize:\n",
        "            tokenizer.normalizer = NFKC()\n",
        "\n",
        "        parameters = {\n",
        "            \"model\": \"SentencePieceBPE\",\n",
        "            \"unk_token\": unk_token,\n",
        "            \"replacement\": replacement,\n",
        "            \"add_prefix_space\": add_prefix_space,\n",
        "            \"dropout\": dropout,\n",
        "        }\n",
        "\n",
        "        super().__init__(tokenizer, parameters)\n",
        "        bos_token = AddedToken(bos_token, lstrip=False, rstrip=False)\n",
        "        eos_token = AddedToken(eos_token, lstrip=False, rstrip=False)\n",
        "        sep_token = AddedToken(sep_token, lstrip=False, rstrip=False)\n",
        "        cls_token = AddedToken(cls_token, lstrip=False, rstrip=False)\n",
        "        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False)\n",
        "        pad_token = AddedToken(pad_token, lstrip=False, rstrip=False)\n",
        "\n",
        "        self.add_special_tokens([\n",
        "            bos_token,\n",
        "            eos_token,\n",
        "            sep_token,\n",
        "            cls_token,\n",
        "            unk_token,\n",
        "            pad_token,\n",
        "        ])\n",
        "\n",
        "    @staticmethod\n",
        "    def from_file(\n",
        "        vocab_filename: str,\n",
        "        merges_filename: Union[str, None],\n",
        "        **kwargs,\n",
        "    ):\n",
        "        vocab, merges = BPE.read_file(vocab_filename, merges_filename)\n",
        "        return BrainBertTokenizer(vocab, merges, **kwargs)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        text: str,\n",
        "        return_tensors: bool = True,\n",
        "        add_special_tokens: str = \"pt\",\n",
        "    ) -> Union[List[int], torch.Tensor]:\n",
        "        \"\"\"\n",
        "        encode text for brainbert.\n",
        "        Args:\n",
        "            text (str): input sentence\n",
        "            return_tensors (str): whether convert list of int to `torch.Tensor` or not\n",
        "            add_special_tokens (bool): whether add <s>, </s> to encoding or not\n",
        "        Returns:\n",
        "            (List[str]): list of token ids\n",
        "            (torch.Tensor): tensor of token ids\n",
        "        \"\"\"\n",
        "\n",
        "        if add_special_tokens:\n",
        "            text = f\"<s>{text}</s>\"\n",
        "\n",
        "        input_ids = self.encode(text).ids\n",
        "\n",
        "        if return_tensors == \"pt\":\n",
        "            input_ids = torch.tensor(input_ids).unsqueeze(0).long()\n",
        "\n",
        "        return input_ids"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a9PbFcV93sG"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syg8_wnO-8By",
        "outputId": "6a4ba19b-ba57-43bf-8289-87c50ba9ed00"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBN5OMgaALC2"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAgDE6uWFg3a"
      },
      "source": [
        "class Train_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "\n",
        "class Test_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz4Fdgq89_IU"
      },
      "source": [
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import XLMRobertaTokenizer, BertTokenizerFast\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "class DataModule(LightningDataModule):\n",
        "\n",
        "    # text_map = ['요약문_연구내용', '과제명']\n",
        "\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.hparams = hparams\n",
        "        # self.dataset=DatasetDict()\n",
        "        self.train_dataset = []\n",
        "        self.val_dataset  = []\n",
        "        self.test_dataset = []\n",
        "\n",
        "        self.text_map = ['과제명', '사업명', '내역사업명', '요약문_한글키워드', '요약문_기대효과', '요약문_연구목표', '요약문_연구내용']\n",
        "        self.random_seed = 45\n",
        "\n",
        "\n",
        "        # korean/multilingual tokenizers [kobert, brainbert, xlm-roberta, albert, .. etc]\n",
        "        if hparams.language == 'ko' or hparams.language == 'multi':\n",
        "          if self.hparams.model_or_tokenizer_name == 'skt/kobert-base-v1':\n",
        "            self.tokenizer = KoBERTTokenizer.from_pretrained(self.hparams.model_or_tokenizer_name)\n",
        "          elif self.hparams.model_or_tokenizer_name == 'hyunwoongko/brainbert-base-ko-kornli':\n",
        "                download_brainbert_tokenizer.from_file(\n",
        "                vocab_filenmae='brainbert.merges.txt', \n",
        "                merges_filename='brainbert.vocam.json'\n",
        "                )\n",
        "\n",
        "                self.tokenizer = BrainBertTokenizer.from_file(\n",
        "                    vocab_filenmae='brainbert.merges.txt', \n",
        "                    merges_filename='brainbert.vocam.json'\n",
        "                )\n",
        "          elif self.hparams.model_or_tokenizer_name == 'xlm-roberta-large':\n",
        "            self.tokenizer = XLMRobertaTokenizer.from_pretrained(self.hparams.model_or_tokenizer_name)\n",
        "          elif self.hparams.model_or_tokenizer_name == 'kykim/albert-kor-base':\n",
        "            self.tokenizer = BertTokenizerFast.from_pretrained(self.hparams.model_or_tokenizer_name)\n",
        "\n",
        "          else :\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.hparams.model_or_tokenizer_name)\n",
        "\n",
        "        # english tokenizers [ ,.. etc]\n",
        "        elif hparams.language == 'en':\n",
        "          pass\n",
        "\n",
        "\n",
        "    def preprocessing(self, pandas_data):\n",
        "        all_col = pandas_data.columns.values.tolist()\n",
        "        # drop other columns except 'label'\n",
        "        for col in all_col :\n",
        "          if not col in self.text_map and col != 'label':\n",
        "            pandas_data = pandas_data.drop(col, axis=1)        \n",
        "        \n",
        "        # training data에서 짧은 순서 -> 긴 순서로 컬럼 위치 변경\n",
        "        if 'label' in pandas_data.columns.values.tolist() :\n",
        "          temp = pandas_data['label']\n",
        "          pandas_data = pandas_data[self.text_map]\n",
        "          pandas_data['label'] = temp\n",
        "\n",
        "        # fillna and delete everything but korean\n",
        "        for col in self.text_map:\n",
        "            pandas_data[col].fillna('NAN', inplace=True)\n",
        "            if self.hparams.language == 'ko' :\n",
        "              pandas_data[col] = pandas_data[col].apply(lambda x:re.sub(\"[^가-힣ㄱ-하-ㅣ]\", \" \",x))\n",
        "        #if 'label' in pandas_data.columns:\n",
        "            # for t5, label is feeded by str\n",
        "            #pandas_data['label'] = pandas_data['label'].apply(str)\n",
        "        return pandas_data\n",
        "\n",
        "    def prepare_data(self, train_or_test):\n",
        "        \"\"\"\n",
        "        train_or_test : one of [train], [test], [train test]\n",
        "        이쪽부분에서 data agument 할 수 있을듯.\n",
        "        \"\"\"\n",
        "        if 'train' not in train_or_test and 'test' not in train_or_test:\n",
        "            raise Exception('please choose one of [train], [test], [train test]')\n",
        "        \n",
        "        # prepare train or test data\n",
        "        if 'train' in train_or_test:\n",
        "            pandas_data = pd.read_csv(self.hparams.data_path  + '/train.csv',index_col=0)\n",
        "            pandas_data = self.preprocessing(pandas_data)\n",
        "\n",
        "            # train과 test의 label 분포를 일정하게 만들기 위해서 sklearn의 train_test_split사용   \n",
        "            train, valid, y_train, y_valid = train_test_split(\n",
        "                                                      pandas_data[self.text_map], \n",
        "                                                      pandas_data['label'], \n",
        "                                                      test_size=self.hparams.train_test_split, \n",
        "                                                      stratify= pandas_data['label'],\n",
        "                                                      random_state=self.random_seed\n",
        "                                                      )\n",
        "            # train['label'] = y_train\n",
        "            # valid['label'] = y_valid\n",
        "            \n",
        "            # self.dataset['train'] = Dataset.from_pandas(train)\n",
        "            # self.dataset['valid'] = Dataset.from_pandas(valid)\n",
        "\n",
        "\n",
        "            \n",
        "            self.train_dataset = Train_Dataset(train, y_train)\n",
        "            self.val_dataset = Train_Dataset(valid, y_valid)\n",
        "\n",
        "          \n",
        "            # 인섭님 코드\n",
        "            # train_valid = Dataset.from_pandas(pandas_data).train_test_split(self.hparams.train_test_split, seed=self.random_seed)\n",
        "            # self.dataset['train'] = train_valid['train']\n",
        "            # self.dataset['valid'] = train_valid['test']\n",
        "\n",
        "        if 'test' in train_or_test:\n",
        "            pandas_data = pd.read_csv(self.hparams.data_path + '/test.csv',index_col=0)\n",
        "            pandas_data = self.preprocessing(pandas_data)\n",
        "            # self.dataset['test'] = Dataset.from_pandas(pandas_data)\n",
        "            self.test_dataset = Test_Dataset(pandas_data)\n",
        "        \n",
        "        \n",
        "        \n",
        "    def setup(self, stage: str):\n",
        "        # convert data and tokenize\n",
        "        # for split in self.dataset.keys():\n",
        "        #     self.dataset[split] = self.dataset[split].map(\n",
        "        #             self.convert_to_features,\n",
        "        #             batched=True,\n",
        "        #             remove_columns = self.dataset[split].column_names,\n",
        "        #         )\n",
        "            # self.dataset[split].set_format(type=\"torch\", columns=self.dataset[split].column_names)\n",
        "        for dataset in [self.train_dataset, self.val_dataset, self.test_dataset] :\n",
        "          for split in dataset[0].keys():\n",
        "              dataset[split] = convert_to_features(dataset[split])\n",
        "            # self.dataset[split].set_format(type=\"torch\", columns=self.dataset[split].column_names)\n",
        "\n",
        "\n",
        "    def convert_to_features(self, example_batch, indices=None):\n",
        "        \"\"\"\n",
        "        example_batch : dict(list)\n",
        "        test_map = ['color','smell','fruit']\n",
        "        test_example_batch = {'fruit':['apple', 'banana'],'color':['red','yellow'],'smell':['bad','good']}\n",
        "        texts_or_text_pairs = [' '.join(i) for i in list(zip(*[test_example_batch[i] for i in test_map]))]\n",
        "        print(texts_or_text_pairs) #['red bad apple', 'yellow good banana']\n",
        "        \"\"\"\n",
        "        '''\n",
        "        # 과제명과 요약문_연구내용을 붙여서 토크나이저에 넣음. self.text_map 설정으로 순서, 추가피쳐 넣을 수 있음.\n",
        "        concatted_texts = [' '.join(i) for i in list(zip(*[example_batch[i] for i in self.text_map]))]\n",
        "        # 토크나이저는 input_ids랑 attentions_mask를 줌. 여기서는 기본배치사이즈 1000일거임\n",
        "        features = self.tokenizer.batch_encode_plus(\n",
        "                concatted_texts, \n",
        "                max_length=self.hparams.max_seq_length, \n",
        "                padding='max_length',\n",
        "                truncation=True\n",
        "            )\n",
        "        print(concatted_texts[0])\n",
        "        \n",
        "\n",
        "        # features = self.tokenizer(\n",
        "        #     concatted_texts,\n",
        "        #     padding='max_length',\n",
        "        #     truncation=True,\n",
        "        #     max_length=self.max_length,\n",
        "        #     return_tensors='pt'\n",
        "        # )\n",
        "\n",
        "\n",
        "        if example_batch.get('label') is not None:\n",
        "            features['labels'] = example_batch.get('label')\n",
        "        \n",
        "        return features\n",
        "        '''\n",
        "        pass"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP5-2I87-BTL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "outputId": "a2d8f8de-6dbb-4a73-f614-e3bb394cc313"
      },
      "source": [
        "data_path = \"/content/drive/MyDrive/dataset/open\"\n",
        "tokenizer_or_model_path = \"skt/kobert-base-v1\"\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--data_path\", required=False, type=str, default=data_path)\n",
        "parser.add_argument(\"--model_or_tokenizer_name\", required=False, type=str, default=tokenizer_or_model_path)\n",
        "parser.add_argument(\"--max_seq_length\", required=False, type=int, default=512)\n",
        "parser.add_argument(\"--train_test_split\", required=False, type=float, default=0.1)\n",
        "parser.add_argument(\"-f\", \"--file\", required=False)\n",
        "# language of dataset or model\n",
        "parser.add_argument(\"--language\", required=False, type=str, default='ko') # [ko, multi, en]\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "dm = DataModule(args)\n",
        "dm.prepare_data(['train','test'])\n",
        "dm.setup('fit')\n",
        "dm.dataset"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading file https://huggingface.co/skt/kobert-base-v1/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/6920ce54223b52af14e36b32047ced34c47ec88ac51f45ce0141aaa1054e3263.7eed87d19282a93a2d45e130f20b4d8e831cbf8e957f1476628fd4ab99ae977f\n",
            "loading file https://huggingface.co/skt/kobert-base-v1/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/skt/kobert-base-v1/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/2ad28172340bc816ccd4ffc7a51682e0c5c89a88a0d618ab40eeb81a3980b356.3db0799720217f7da35e92d033f167ac40c8d2c02fa035130b7bb070f6355074\n",
            "loading file https://huggingface.co/skt/kobert-base-v1/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ebd8df8703bef77849f188d10c4ca994fd45a1b41518401a08c13487ef55c723.55c5c51d9ae1a9f730238c32bd0fa05b12cd7d99d757ee0e6accc4c6e4085f40\n",
            "loading file https://huggingface.co/skt/kobert-base-v1/resolve/main/tokenizer.json from cache at None\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-9d36ed3205ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-3c9461ff1560>\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# self.dataset[split].set_format(type=\"torch\", columns=self.dataset[split].column_names)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m               \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m# self.dataset[split].set_format(type=\"torch\", columns=self.dataset[split].column_names)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-84dbfc0a5090>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-84dbfc0a5090>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7Mzn_yoI79y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2247f5be-3390-4ba1-d1e4-a36ba22b560d"
      },
      "source": [
        "dm.dataset['train']['input_ids'][0] # original"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    2, 4897, 6278, 7675, 6664, 4177, 6241, 5468, 2255, 6516, 6242,\n",
              "        3728, 4665, 7533, 6797, 4158, 2184,  517, 5468, 7147, 6107,  839, 4329,\n",
              "        7663, 7941, 6517, 7067, 6573, 4329, 7276, 6064, 6517, 7067, 6573, 4897,\n",
              "        6278, 7675, 6664, 3574, 6516, 5536, 2255, 6516, 6241, 4665, 7533, 6797,\n",
              "        4158,  517, 5468, 7147, 6107, 4158, 2658, 7003, 7941, 6079, 4066, 6305,\n",
              "        6517, 1748, 2184,  972, 5144, 6983, 3533, 7422, 5575, 5126, 2184, 1289,\n",
              "        7902, 7921, 1292, 3600, 7078, 3801, 2641,  993, 5859, 7942, 3206, 5561,\n",
              "        7831,  882, 3592, 6896, 3574, 7003, 7828, 3954, 5678, 2801, 5536, 6896,\n",
              "        1682, 3376, 6853, 7202, 5170, 6983, 4382, 7095, 3401, 2184, 4619, 6896,\n",
              "        2165, 5170, 6079, 3802, 3809, 2184, 3581, 1815, 6904, 3686, 3728, 4158,\n",
              "        7096, 2683, 7941, 5894, 4809, 5887, 3876, 3503, 6904, 1205, 2089, 3036,\n",
              "        7227, 7100, 1864, 3139,  882,  517, 7042, 6456,  517, 7998, 6139, 3009,\n",
              "        7088, 2768, 7794, 2833, 7095, 2892, 2184, 2991, 2792, 7078, 1830, 7199,\n",
              "        7659, 5176, 6730, 2132, 5330, 3358, 7750, 7100, 4946,  993, 7500, 6079,\n",
              "        6122, 4910, 7659, 6037, 6438, 1815,  882, 5468, 1086, 3500, 3954, 6354,\n",
              "        7078, 1399, 5439, 3878, 3724,  517, 5770, 7310, 1659, 7921,  882, 7828,\n",
              "        4665, 7533, 6797, 4158, 2184,  517, 5468, 7147, 6107, 6116,  839, 7788,\n",
              "        7147, 4965, 4402, 5712, 5859, 6900, 3574, 7239,  517, 6079, 3172,  517,\n",
              "        6983, 5100, 7842, 2287, 6853, 7815, 2255, 4159,  839, 7788, 3686, 1830,\n",
              "        7199, 7659, 4158, 6896, 4004, 7788, 7147, 4965, 4402, 5712, 5859, 6900,\n",
              "        4257, 1264, 6573,  517, 7948, 6573, 5765, 7096, 3508, 7828, 3954, 5678,\n",
              "        2801, 5536, 7088, 2734, 6356, 7815, 1264, 6573,  517, 7948, 6573, 5765,\n",
              "        7096, 3508, 7828, 2255, 4177, 6242, 5126, 7788, 3140, 6574, 3139, 6573,\n",
              "        3860, 3009, 2846, 7941, 4977,  992, 2658, 5561, 2592, 6116,  903, 7848,\n",
              "        2408,  938, 3574, 7239, 6896, 4004, 7828, 2592, 5937, 2043, 5384, 3871,\n",
              "        7075, 3725, 3586, 7828, 3336, 2592, 5760,  898, 5898, 3156, 7082, 3574,\n",
              "        7239, 3468, 2255, 6896, 4004, 7828, 2592, 5760,  873, 3272, 3036, 7227,\n",
              "        7100, 3724, 3954, 5678, 2801, 5536, 7088, 5146, 7815, 4525, 4773, 6026,\n",
              "        6896, 1971, 4906, 4501, 7542, 6138, 5468,  517, 5468, 7158,  839, 7788,\n",
              "        3686, 4754, 2641, 7942, 4556, 7147, 4965,    3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "tSSndM4xs_Qn",
        "outputId": "939f70ee-8e9d-41a0-bbfb-7a8a896980ec"
      },
      "source": [
        "dm.tokenizer.decode(dm.dataset['train']['input_ids'][0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][CLS] 프로바이오틱스 조성물과 발효산물을 이용한 코코아 제품 및 과자류 개발 지역특화산업육성 지역주력산업육성 프로바이오틱스 유산균 발효산물 코코아 제품 과자류 제품 상용화로 전후방산업 동반성장 및 경제 활성화와 원천기술 확보 및 기술혁신형 기업 육성으로 인한 산업 고도화를 야기한다 건강 유지에 유용한 장내 세균에 대한 영양적 효과와 질병의 예방 및 치료에 미치는 효과로 인해 일본 및 유럽 등에서는 이를 이용한 제품이 상품화되어 판매되고 있으나 우리나라에서는 그렇지 못한 실정이다 또한 안전 건강 웰빙 힐링 식품을 선호하는 소비자의 수요 및 시장 성장으로 디저트 후식 문화가 열풍이다 하지만 고칼로리 피부트러블 등 건강과 관련한 우려가 장벽으로 나타나고 있으며 이에 니즈 대응형 건강한 코코아 제품 및 과자류를 개발하고자 한다 차년도에는 유제품 로 알려진 와 혼합 배양하여 발효 제품을 개발하고 이를 디저트 제품에 적용하고자 한다 차년도에는 중 기능성 활성능이 우수한 장내 세균을 선별하여 기능성 활성능이 우수한 발효 조성물을 확보하고 안정성과 안전성 있는 식품 소재화 할 계획이다 상기 사례를 검토해 본 결과 유제품에 적용한 사례들은 몇건 있었으나 이와 유사한 연구 사례는 검색되지 않았으며 유제품 외 발효에 적용한 사례는 거의 없는 실정이다 이에 장내 세균을 활용하여 최근 트랜드에 맞는 프리미엄 초콜릿과 과자를 개발하고 이를 통한 산업화를 추진하고자 한다[SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IKJdim3lYx4"
      },
      "source": [
        "\n",
        "# replace '/' with '-' to save\n",
        "tokenizer_or_model_path = \"skt/kobert-base-v1\"\n",
        "drive_path = tokenizer_or_model_path.replace('/', '-')\n",
        "\n",
        "path = \"/content/drive/MyDrive/dataset/open/tokenized_dataset/\"\n",
        "os.mkdir(path + drive_path)\n",
        "\n",
        "# save tokenized dataset (앞으로 데이터 여러 버전이 생길텐데, 데이터 버전도 경로에 추가하면 안헷갈릴듯?)\n",
        "dm.dataset.save_to_disk(\"/content/drive/MyDrive/dataset/open/tokenized_dataset/\" +drive_path)\n",
        "\n",
        "\n",
        "## 여기서 부터는 load 임. \n",
        "\n",
        "# load tokenized dataset    (앞으로 데이터 여러 버전이 생길텐데, 데이터 버전도 경로에 추가하면 안헷갈릴듯?)\n",
        "from datasets import load_from_disk, load_from_disk\n",
        "train_dataset = load_from_disk(path + drive_path + \"/train\")\n",
        "valid_dataset = load_from_disk(path + drive_path + \"/valid\")\n",
        "test_dataset = load_from_disk(path + drive_path + \"/test\")\n",
        "\n",
        "# make DatasetDict from loaded tokenized dataset\n",
        "dataset = DatasetDict()\n",
        "dataset['train'] = train_dataset\n",
        "dataset['valid'] = valid_dataset\n",
        "dataset['test'] = test_dataset"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgbPZIggoJT-",
        "outputId": "3c784619-76f8-48f2-902c-825f69d7a990"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n",
              "        num_rows: 156873\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n",
              "        num_rows: 17431\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['attention_mask', 'input_ids', 'token_type_ids'],\n",
              "        num_rows: 43576\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUalC_kbpAN-",
        "outputId": "4de2baf8-ebd6-4b28-fa29-4d519eafc11e"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n",
              "        num_rows: 156873\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n",
              "        num_rows: 17431\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['attention_mask', 'input_ids', 'token_type_ids'],\n",
              "        num_rows: 43576\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ1z5aNU-BLr",
        "outputId": "1def24a7-45e8-471a-ce17-5175adce45cf"
      },
      "source": [
        "len(train_pd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "174304"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "wSnxdMt_Ld7i",
        "outputId": "883bab32-3e84-4162-eecf-8a60e0debe9c"
      },
      "source": [
        "train_pd = preprocessing(train_pd)\n",
        "\n",
        "train_pd.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>과제명</th>\n",
              "      <th>사업명</th>\n",
              "      <th>내역사업명</th>\n",
              "      <th>요약문_한글키워드</th>\n",
              "      <th>요약문_기대효과</th>\n",
              "      <th>요약문_연구목표</th>\n",
              "      <th>요약문_연구내용</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>유전정보를 활용한 새로운 해충 분류군 동정기술 개발</td>\n",
              "      <td>농업기초기반연구</td>\n",
              "      <td>농산물안전성연구</td>\n",
              "      <td>뉴클레오티드 염기서열  분자마커  종 동정  침샘  전사체</td>\n",
              "      <td>새로운 돌발 및 외래해충의 신속  정확한 동정법 향상     돌발 및 외래해충의...</td>\n",
              "      <td>새로운 해충분류군의 동정기술 개발 및 유입확산 추적</td>\n",
              "      <td>가  외래 및 돌발해충의 발생조사 및 종 동정         대상해충   최근 새...</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...</td>\n",
              "      <td>이공학학술연구기반구축</td>\n",
              "      <td>지역대학우수과학자지원사업  년  년</td>\n",
              "      <td>대장암 항암제 내성 세포사멸 유전자발굴</td>\n",
              "      <td>내성 특이적 표적분자를 발굴하고  이를 이용한       효과 증진...</td>\n",
              "      <td>최종목표         감수성 표적 유전자를 발굴하고 내성제어 기전을 연구  발굴된...</td>\n",
              "      <td>차년도              를 통한 선천적       내성 표적 후보 유전자 ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>비목질계 셀룰로오스 식물자원을 활용한 기능성 부직포 및 고부가가치 뷰티케어     ...</td>\n",
              "      <td>중소기업기술혁신개발</td>\n",
              "      <td>혁신기업기술개발</td>\n",
              "      <td>기능성 샐룰로오스 파이버 천연섬유 기능성 부직포 뷰티     케어 제품 미용 솜</td>\n",
              "      <td>국내 독자적인 비목질계 셀룰로오스 자원의 파이버 및 부직포 제조 등의 기술 확보...</td>\n",
              "      <td>식물계자원 정련 및 최적 신서란 파이버 기초연구 개발     소비자 및 바이어들...</td>\n",
              "      <td>식물계자원 정련 및 최적 신서란 파이버 기초연구 개발   -           ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>소화기 암 진단용 분자영상 형광프로브 개발</td>\n",
              "      <td>창업성장기술개발</td>\n",
              "      <td>창업사업화연계과제</td>\n",
              "      <td>분자 진단 형광 조영제 프로브 항체 대장암</td>\n",
              "      <td>암 진단기술의 차별성  소화기 암 특이 프로브 개발   - 최근 체외진단시장은 ...</td>\n",
              "      <td>암특이적 바이오마커 발굴 및 바이오마커에 대한 프로브 개발      소화기 암...</td>\n",
              "      <td>소화기 암 진단용 분자영상 형광프로브 개발   - 국소 도포형 소화기 암 분자 ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>위암환자의 항암제반응예측을 위한      발현검사</td>\n",
              "      <td>이공학개인기초연구지원</td>\n",
              "      <td>기본연구지원</td>\n",
              "      <td>제자리부합법 조직미세배열 마이크로    위암 항암제반응 젊은 연령 가족성 위암</td>\n",
              "      <td>-본 연구는 파라핀보관조직에서                      로      및...</td>\n",
              "      <td>수술이 불가능한 위암환자는 생존기간은   개월 안팎에 지나지 않고  항암화학요법에 ...</td>\n",
              "      <td>-                      검사의 정확성을 확인하기 위해 위암세포주 ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     과제명  ... label\n",
              "index                                                     ...      \n",
              "0                           유전정보를 활용한 새로운 해충 분류군 동정기술 개발  ...    24\n",
              "1      대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...  ...     0\n",
              "2      비목질계 셀룰로오스 식물자원을 활용한 기능성 부직포 및 고부가가치 뷰티케어     ...  ...     0\n",
              "3                                소화기 암 진단용 분자영상 형광프로브 개발  ...     0\n",
              "4                            위암환자의 항암제반응예측을 위한      발현검사  ...     0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4pbt5GTLdwS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBS6YixRo6F0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aac8Hn6mb_p",
        "outputId": "39a58f17-cafe-4d5e-94e1-6624740efad4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-cvv4zxkb/kobert-tokenizer_772b1e5013ff488b903264c5733518f9\n",
            "  Running command git clone -q https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-cvv4zxkb/kobert-tokenizer_772b1e5013ff488b903264c5733518f9\n",
            "Building wheels for collected packages: kobert-tokenizer\n",
            "  Building wheel for kobert-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert-tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4627 sha256=3e05a2cf728aa2a967c5bc5ccabddfdec94ddf79d9513f4736da9e41fbe8efd7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qpypxork/wheels/10/b4/d9/cb627bbfaefa266657b0b4e8127f7bf96d27376fa1a23897b4\n",
            "Successfully built kobert-tokenizer\n",
            "Installing collected packages: kobert-tokenizer\n",
            "Successfully installed kobert-tokenizer-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvhH7sg-B40"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQEEP5Meg2Tb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtqsgQzYf_kE"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from transformers import MT5EncoderModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "\n",
        "class Mt5ForSequenceClassification(MT5EncoderModel):\n",
        "    \"\"\"\n",
        "    # modify from DistilBertForSequenceClassification\n",
        "    from transformers import MT5Config\n",
        "\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
        "\n",
        "    config = MT5Config.from_pretrained(\"google/mt5-small\",\n",
        "                                    vocab_size=tokenizer.vocab_size,\n",
        "                                    num_labels=22,\n",
        "                                    pad_token_id=tokenizer.pad_token_id)\n",
        "    model = Mt5ForSequenceClassification(config)\n",
        "\n",
        "    article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n",
        "    inputs = tokenizer(article, return_tensors=\"pt\")\n",
        "    labels = torch.tensor([1]).unsqueeze(0)\n",
        "    outputs = model(**inputs)\n",
        "    print(outputs.keys()) # odict_keys(['logits'])\n",
        "    outputs = model(**inputs, labels=labels)\n",
        "    print(outputs.keys()) # odict_keys(['loss', 'logits'])\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        config.update({\"dim\":config.d_model,\n",
        "                       \"seq_classif_dropout\":0.1})\n",
        "        #help(config.update)\n",
        "        self.config = config\n",
        "        #print(config)\n",
        "\n",
        "        #self.distilbert = DistilBertModel(config)\n",
        "        self.model = MT5EncoderModel(config)\n",
        "        \n",
        "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
        "        self.classifier = nn.Linear(config.dim, config.num_labels)\n",
        "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        \n",
        "        model_output = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        hidden_state = model_output[0]  # (bs, seq_len, dim)\n",
        "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
        "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
        "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
        "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
        "        logits = self.classifier(pooled_output)  # (bs, num_labels)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + model_output[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=model_output.hidden_states,\n",
        "            attentions=model_output.attentions,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjHbvVEBiCZp"
      },
      "source": [
        "def mkdirForSave(model_name, path, dataset_number):\n",
        "  # replace '/' with '-' to save\n",
        "  drive_path = model_name.replace('/', '-')\n",
        "  \n",
        "  new_dir = path + drive_path + '/DataVersion-' + str(dataset_number)\n",
        "  if not os.path.exists(new_dir):\n",
        "    os.makedirs(new_dir)\n",
        "    print(\"your '{}' folder has been successfully created.\".format(new_dir))\n",
        "  else:\n",
        "    print(\"'{}' folder already exists.\".format(new_dir))\n",
        " "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDYqqnAFkBGO",
        "outputId": "9ce8a2a3-05a2-4162-fe45-0e016610367a"
      },
      "source": [
        "mkdirForSave(\n",
        "    model_name=tokenizer_or_model_path, \n",
        "    path=\"/content/drive/MyDrive/dataset/open/models/\",\n",
        "    dataset_number=1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "your '/content/drive/MyDrive/dataset/open/models/skt-kobert-base-v1/DataVersion-1' folder has been successfully created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tb-JIrB-FmL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96c2d84a-8e1b-4e8c-e406-2392431126be"
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertModel\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "\n",
        "  config= BertModel.from_pretrained(\n",
        "      args.model_or_tokenizer_name,\n",
        "      vocab_size=dm.tokenizer.vocab_size,\n",
        "      num_labels=22,\n",
        "      pad_token_id=dm.tokenizer.pad_token_id\n",
        "  )\n",
        "\n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      'skt/kobert-base-v1', \n",
        "      pad_token_id = tokenizer.pad_token_id,\n",
        "      num_labels=22)\n",
        "\n",
        "\n",
        "  # training\n",
        "  # fine-tuning\n",
        "  \n",
        "  training_args = TrainingArguments(\n",
        "    output_dir=\"./\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=3, # number of training epochs\n",
        "    # per_device_train_batch_size=32, # batch size for training\n",
        "    # per_device_eval_batch_size=64,  # batch size for evaluation\n",
        "    # eval_steps = 400, # Number of update steps between two evaluations.\n",
        "    # save_steps=800, # after # steps model is saved\n",
        "    # warmup_steps=500,# number of warmup steps for learning rate scheduler)\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # config = MT5Config.from_pretrained(\"google/mt5-small\",\n",
        "    #                                 vocab_size=tokenizer.vocab_size,\n",
        "    #                                 num_labels=22,\n",
        "    #                                 pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      num_labels = 22,\n",
        "      train_dataset=dm.dataset['train'],\n",
        "      eval_dataset=dm.dataset['valid'],\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.evaluate()\n",
        "  trainer.save_model('./')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/skt/kobert-base-v1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/b8898dabd49ed32401ee6a6bc5eb011f12728750b44d08b151acf270bf1732ca.1007ab583c49854e3c65c61288a980ae4d25a4bbfa51b51915ec1772f02f992d\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\",\n",
            "    \"14\": \"LABEL_14\",\n",
            "    \"15\": \"LABEL_15\",\n",
            "    \"16\": \"LABEL_16\",\n",
            "    \"17\": \"LABEL_17\",\n",
            "    \"18\": \"LABEL_18\",\n",
            "    \"19\": \"LABEL_19\",\n",
            "    \"20\": \"LABEL_20\",\n",
            "    \"21\": \"LABEL_21\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"kobert_version\": 1.0,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_14\": 14,\n",
            "    \"LABEL_15\": 15,\n",
            "    \"LABEL_16\": 16,\n",
            "    \"LABEL_17\": 17,\n",
            "    \"LABEL_18\": 18,\n",
            "    \"LABEL_19\": 19,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_20\": 20,\n",
            "    \"LABEL_21\": 21,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.9.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 8002\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/skt/kobert-base-v1/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/29f85cb7266d82cd889c6f5db4d439fb71b42c237c56f56fce78e950c7d4a2e5.b39ec656f15b262c43c61e29d10204da76efaf1f5535892e2c90431065f17dba\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "loading configuration file https://huggingface.co/skt/kobert-base-v1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/b8898dabd49ed32401ee6a6bc5eb011f12728750b44d08b151acf270bf1732ca.1007ab583c49854e3c65c61288a980ae4d25a4bbfa51b51915ec1772f02f992d\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\",\n",
            "    \"14\": \"LABEL_14\",\n",
            "    \"15\": \"LABEL_15\",\n",
            "    \"16\": \"LABEL_16\",\n",
            "    \"17\": \"LABEL_17\",\n",
            "    \"18\": \"LABEL_18\",\n",
            "    \"19\": \"LABEL_19\",\n",
            "    \"20\": \"LABEL_20\",\n",
            "    \"21\": \"LABEL_21\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"kobert_version\": 1.0,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_14\": 14,\n",
            "    \"LABEL_15\": 15,\n",
            "    \"LABEL_16\": 16,\n",
            "    \"LABEL_17\": 17,\n",
            "    \"LABEL_18\": 18,\n",
            "    \"LABEL_19\": 19,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_20\": 20,\n",
            "    \"LABEL_21\": 21,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.9.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 8002\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/skt/kobert-base-v1/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/29f85cb7266d82cd889c6f5db4d439fb71b42c237c56f56fce78e950c7d4a2e5.b39ec656f15b262c43c61e29d10204da76efaf1f5535892e2c90431065f17dba\n",
            "All model checkpoint weights were used when initializing BertModel.\n",
            "\n",
            "All the weights of BertModel were initialized from the model checkpoint at skt/kobert-base-v1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "***** Running training *****\n",
            "  Num examples = 156873\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 58830\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-add516b3fbac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m   )\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1278\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1803\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1806\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1532\u001b[0m         )\n\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m         )\n\u001b[1;32m    991\u001b[0m         encoder_outputs = self.encoder(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpzKCBwL-JLg",
        "outputId": "09823979-cb5f-4125-89d8-a85cec5ca85a"
      },
      "source": [
        "# classifier = pipeline('sentiment-analysis', model=)\n",
        "\n",
        "# model_name = \n",
        "# pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained('kykim/bert-kor-base')\n",
        "\n",
        "inputs = tokenizer(\" , , , 인공지능 , AI\",\n",
        "                   padding='max_length',\n",
        "                   truncation=True,\n",
        "                   max_length=20)\n",
        "\n",
        "# print(tokenizer.decode(inputs['input_ids']))\n",
        "# print(inputs)\n",
        "for ids in inputs['input_ids'] :\n",
        "  print(tokenizer.decode(ids), end='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS]\n",
            ",\n",
            ",\n",
            ",\n",
            "인공지능\n",
            ",\n",
            "ai\n",
            "[SEP]\n",
            "[PAD]\n",
            "[PAD]\n",
            "[PAD]\n",
            "[PAD]\n",
            "[PAD]\n",
            "[PAD]\n",
            "[PAD]\n",
            "[PAD]\n",
            "[PAD]\n",
            "[PAD]\n",
            "[PAD]\n",
            "[PAD]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1O_CGUy-JJR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwdRB2QZ-JHE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vNssf7b-JCR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Of-nB00-I_5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}